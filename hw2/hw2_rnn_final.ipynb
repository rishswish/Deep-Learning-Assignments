{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtcBjMq7YV3f"
      },
      "source": [
        "\n",
        "\n",
        "# Homework 2 - Recurrent Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn-cOk1iZTtR"
      },
      "source": [
        "In this part of the homework we are going to work with Recurrent Neural Networks, in particular GRU. One of the greatest things that Recurrent Neural Networks can do when working with sequences is retaining data from several timesteps in the past. We are going to explore that property by constructing an 'echo' Recurrent Neural Network.\n",
        "\n",
        "The goal here is to make a model that given a sequence of letters or digits will output that same sequence, but with a certain delay. Let's say the input is a string 'abacaba', we want the model to not output anything for 3 steps (delay length), and then output the original string step by step, except the last 3 characters. So, target output is then 'XXXabac', where 'X' is empty output.\n",
        "\n",
        "This is similar to [this notebook](https://github.com/Atcold/pytorch-Deep-Learning/blob/master/09-echo_data.ipynb) (which you should refer to when doing this assignment), except we're working not with a binary string, but with a sequence of integers between 0 and some N. In our case N is 26, which is the number of letters in the alphabet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npLlE973as6x"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "Let's implement the dataset. In our case, the data is basically infinite, as we can always generate more examples on the fly, so there's no need to load it from disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkEEMyvzIMRx"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "import torch\n",
        "\n",
        "# Max value of the generated integer. 26 is chosen becuase it's\n",
        "# the number of letters in English alphabet.\n",
        "N = 26\n",
        "\n",
        "\n",
        "def idx_to_onehot(x, k=N+1):\n",
        "  \"\"\" Converts the generated integers to one-hot vectors \"\"\"\n",
        "  ones = torch.sparse.torch.eye(k)\n",
        "  shape = x.shape\n",
        "  res = ones.index_select(0, x.view(-1).type(torch.int64))\n",
        "  return res.view(*shape, res.shape[-1])\n",
        "\n",
        "\n",
        "class EchoDataset(torch.utils.data.IterableDataset):\n",
        "\n",
        "  def __init__(self, delay=4, seq_length=15, size=1000):\n",
        "    self.delay = delay\n",
        "    self.seq_length = seq_length\n",
        "    self.size = size\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.size\n",
        "\n",
        "  def __iter__(self):\n",
        "    \"\"\" Iterable dataset doesn't have to implement __getitem__.\n",
        "        Instead, we only need to implement __iter__ to return\n",
        "        an iterator (or generator).\n",
        "    \"\"\"\n",
        "    for _ in range(self.size):\n",
        "      seq = torch.tensor([random.choice(range(1, N + 1)) for i in range(self.seq_length)], dtype=torch.int64)\n",
        "      result = torch.cat((torch.zeros(self.delay), seq[:self.seq_length - self.delay])).type(torch.int64)\n",
        "      yield seq, result\n",
        "\n",
        "DELAY = 4\n",
        "DATASET_SIZE = 200000\n",
        "ds = EchoDataset(delay=DELAY, size=DATASET_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNrZqYURcKSl"
      },
      "source": [
        "## Model\n",
        "\n",
        "Now, we want to implement the model. For our purposes, we want to use GRU. The architecture consists of GRU and a decoder. Decoder is responsible for decoding the GRU hidden state to yield a predicting for the next output. The parts you are responsible for filling with your code are marked with `TODO`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nigN_o4Mb9Nx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GRUMemory(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        # GRU: input size = N+1 (one-hot length), hidden size = hidden_size\n",
        "        self.gru = nn.GRU(input_size=N+1, hidden_size=hidden_size, batch_first=True)\n",
        "        # Decoder: map hidden state → N+1 logits\n",
        "        self.decoder = nn.Linear(hidden_size, N+1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch_size, seq_length, N+1) one-hot inputs\n",
        "        returns: logits (batch_size, seq_length, N+1)\n",
        "        \"\"\"\n",
        "        # GRU pass\n",
        "        out, _ = self.gru(x)   # out: (batch_size, seq_length, hidden_size)\n",
        "        # Decode each timestep\n",
        "        logits = self.decoder(out)  # (batch_size, seq_length, N+1)\n",
        "        return logits\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def test_run(self, s):\n",
        "        \"\"\"\n",
        "        s: input string (letters a-z)\n",
        "        returns: output string with echo delay learned\n",
        "        \"\"\"\n",
        "        # map chars → indices\n",
        "        idxs = torch.tensor([ord(c) - ord('a') + 1 for c in s], dtype=torch.int64)\n",
        "        onehot = idx_to_onehot(idxs.unsqueeze(0))  # shape (1, seq_len, N+1)\n",
        "\n",
        "        # run through network\n",
        "        logits = self.forward(onehot)\n",
        "        preds = torch.argmax(F.log_softmax(logits, dim=-1), dim=-1)  # (1, seq_len)\n",
        "\n",
        "        # convert back indices → chars\n",
        "        result = []\n",
        "        for i in preds[0]:\n",
        "            if i.item() == 0:\n",
        "                result.append(\" \")   # 0 = blank\n",
        "            else:\n",
        "                result.append(chr(ord('a') + i.item() - 1))\n",
        "        return \"\".join(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9whwmVu9OIx"
      },
      "source": [
        "## Training\n",
        "Below you need to implement the training of the model. We give you more freedom as for the implementation. The two limitations are that it has to execute within 10 minutes, and that error rate should be below 1%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUZkeRnVTNzG"
      },
      "outputs": [],
      "source": [
        "def test_model(model, sequence_length=15):\n",
        "  \"\"\"\n",
        "  This is the test function that runs 100 different strings through your model,\n",
        "  and checks the error rate.\n",
        "  \"\"\"\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  for i in range(500):\n",
        "    s = ''.join([random.choice(string.ascii_lowercase) for i in range(random.randint(15, 25))])\n",
        "    result = model.test_run(s)\n",
        "    for c1, c2 in zip(s[:-DELAY], result[DELAY:]):\n",
        "      correct += int(c1 == c2)\n",
        "    total += len(s) - DELAY\n",
        "\n",
        "  return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lV9BscxCCAI",
        "outputId": "0f536739-3d6f-4e72-8c3f-80bf8b42bcd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tests passed ✅ | duration: 73.70s | accuracy: 1.000\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# ----------------------------\n",
        "# Model setup\n",
        "# ----------------------------\n",
        "HIDDEN_SIZE = 256\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 8\n",
        "LR = 1e-3\n",
        "SEQ_LEN = 15\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "train_ds = EchoDataset(delay=DELAY, seq_length=SEQ_LEN, size=50000)\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE)\n",
        "\n",
        "\n",
        "model = GRUMemory(hidden_size=HIDDEN_SIZE).to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "# ----------------------------\n",
        "# Training loop\n",
        "# ----------------------------\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    for seq, target in train_loader:\n",
        "        x = idx_to_onehot(seq, k=N+1).float().to(DEVICE)\n",
        "        y = target.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits.view(-1, N+1), y.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "end_time = time.time()\n",
        "duration = end_time - start_time\n",
        "\n",
        "# ----------------------------\n",
        "# Final test\n",
        "# ----------------------------\n",
        "accuracy = test_model(model)\n",
        "\n",
        "assert duration < 600, f'execution took {duration:.2f} seconds, which is longer than 10 mins'\n",
        "assert accuracy > 0.99, f'accuracy is too low, got {accuracy:.3f}, need 0.99'\n",
        "print(f'tests passed ✅ | duration: {duration:.2f}s | accuracy: {accuracy:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sB0EVNBtDhpN"
      },
      "source": [
        "## Variable delay model\n",
        "\n",
        "Now, to make this more complicated, we want to have varialbe delay. So, now, the goal is to transform a sequence of pairs (character, delay) into a character sequence with given delay. Delay is constant within one sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3i_iwX_AEOCH"
      },
      "source": [
        "### Dataset\n",
        "As before, we first implement the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4G5b8kuEUEd"
      },
      "outputs": [],
      "source": [
        "class VariableDelayEchoDataset(torch.utils.data.IterableDataset):\n",
        "\n",
        "  def __init__(self, max_delay=8, seq_length=20, size=1000):\n",
        "    self.max_delay = max_delay\n",
        "    self.seq_length = seq_length\n",
        "    self.size = size\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.size\n",
        "\n",
        "  def __iter__(self):\n",
        "    for _ in range(self.size):\n",
        "      seq = torch.tensor([random.choice(range(1, N + 1)) for i in range(self.seq_length)], dtype=torch.int64)\n",
        "      delay = random.randint(0, self.max_delay)\n",
        "      result = torch.cat((torch.zeros(delay), seq[:self.seq_length - delay])).type(torch.int64)\n",
        "      yield seq, delay, result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTRVOND3HEJZ"
      },
      "source": [
        "### Model\n",
        "\n",
        "And the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYolFIB8Hg0U"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class VariableDelayGRUMemory(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size, max_delay):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.max_delay = max_delay\n",
        "\n",
        "        # GRU input size = alphabet one-hot size (N+1) + delay embedding\n",
        "        self.delay_embed = nn.Embedding(max_delay + 1, 8)   # small delay embedding\n",
        "        self.gru = nn.GRU(input_size=(N+1) + 8,\n",
        "                          hidden_size=hidden_size,\n",
        "                          batch_first=True)\n",
        "        self.decoder = nn.Linear(hidden_size, N+1)\n",
        "\n",
        "    def forward(self, x, delays):\n",
        "        \"\"\"\n",
        "        x: (batch, seq_len, N+1)\n",
        "        delays: (batch,) integer delays\n",
        "        returns: logits (batch, seq_len, N+1)\n",
        "        \"\"\"\n",
        "        # embed delays\n",
        "        delay_emb = self.delay_embed(delays)  # (batch, 8)\n",
        "\n",
        "        # repeat delay embedding across time steps\n",
        "        delay_expanded = delay_emb.unsqueeze(1).expand(-1, x.size(1), -1)  # (batch, seq_len, 8)\n",
        "\n",
        "        # concat with input\n",
        "        x_cat = torch.cat([x, delay_expanded], dim=-1)  # (batch, seq_len, N+1+8)\n",
        "\n",
        "        out, _ = self.gru(x_cat)  # (batch, seq_len, hidden_size)\n",
        "        logits = self.decoder(out)  # (batch, seq_len, N+1)\n",
        "        return logits\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def test_run(self, s, delay):\n",
        "        \"\"\"\n",
        "        s: input string (letters a-z)\n",
        "        delay: int, chosen delay\n",
        "        returns: output string\n",
        "        \"\"\"\n",
        "        # map chars to indices\n",
        "        idxs = torch.tensor([ord(c) - ord('a') + 1 for c in s], dtype=torch.int64).unsqueeze(0)\n",
        "        onehot = idx_to_onehot(idxs)  # (1, seq_len, N+1)\n",
        "\n",
        "        # delay as tensor\n",
        "        d = torch.tensor([delay], dtype=torch.int64)\n",
        "\n",
        "        # forward pass\n",
        "        logits = self.forward(onehot.float(), d)\n",
        "        preds = torch.argmax(F.log_softmax(logits, dim=-1), dim=-1)[0]\n",
        "\n",
        "        # convert indices → chars\n",
        "        result = []\n",
        "        for i in preds:\n",
        "            if i.item() == 0:\n",
        "                result.append(\" \")\n",
        "            else:\n",
        "                result.append(chr(ord('a') + i.item() - 1))\n",
        "        return \"\".join(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riu3qHWgKjsx"
      },
      "source": [
        "### Train\n",
        "\n",
        "As before, you're free to do what you want, as long as training finishes within 10 minutes and accuracy is above 0.99 for delays between 0 and 8."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FZHojnGO3aw"
      },
      "outputs": [],
      "source": [
        "def test_variable_delay_model(model, seq_length=20):\n",
        "  \"\"\"\n",
        "  This is the test function that runs 100 different strings through your model,\n",
        "  and checks the error rate.\n",
        "  \"\"\"\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  for i in range(500):\n",
        "    s = ''.join([random.choice(string.ascii_lowercase) for i in range(seq_length)])\n",
        "    d = random.randint(0, model.max_delay)\n",
        "    result = model.test_run(s, d)\n",
        "    if d > 0:\n",
        "      z = zip(s[:-d], result[d:])\n",
        "    else:\n",
        "      z = zip(s, result)\n",
        "    for c1, c2 in z:\n",
        "      correct += int(c1 == c2)\n",
        "    total += len(s) - d\n",
        "\n",
        "  return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJ18Ef6vKi4s",
        "outputId": "122bab2b-e4ef-4beb-c3ce-94622ca7876d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/8 | Loss: 1.4997 | Test Acc: 62.30%\n",
            "Epoch 2/8 | Loss: 0.5572 | Test Acc: 81.64%\n",
            "Epoch 3/8 | Loss: 0.2897 | Test Acc: 89.37%\n",
            "Epoch 4/8 | Loss: 0.1746 | Test Acc: 94.38%\n",
            "Epoch 5/8 | Loss: 0.1057 | Test Acc: 96.72%\n",
            "Epoch 6/8 | Loss: 0.0671 | Test Acc: 97.73%\n",
            "Epoch 7/8 | Loss: 0.0427 | Test Acc: 98.64%\n",
            "Epoch 8/8 | Loss: 0.0294 | Test Acc: 99.17%\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "executing took 824.64s, which is longer than 10 mins",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3758254243.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mfinal_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_variable_delay_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSEQ_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mduration\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'executing took {duration:.2f}s, which is longer than 10 mins'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mfinal_acc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'accuracy is too low, got {final_acc:.3f}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"✅ tests passed | duration {duration:.2f}s | accuracy {final_acc:.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: executing took 824.64s, which is longer than 10 mins"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# ----------------------------\n",
        "# Hyperparameters\n",
        "# ----------------------------\n",
        "MAX_DELAY = 8\n",
        "SEQ_LENGTH = 20\n",
        "HIDDEN_SIZE = 256\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10\n",
        "LR = 1e-3\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ----------------------------\n",
        "# Dataset & Loader\n",
        "# ----------------------------\n",
        "train_ds = VariableDelayEchoDataset(max_delay=MAX_DELAY, seq_length=SEQ_LENGTH, size=100000)\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, drop_last=True)\n",
        "\n",
        "# ----------------------------\n",
        "# Model, Loss, Optimizer\n",
        "# ----------------------------\n",
        "model = VariableDelayGRUMemory(hidden_size=HIDDEN_SIZE, max_delay=MAX_DELAY).to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "# ----------------------------\n",
        "# Training Loop\n",
        "# ----------------------------\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for seq, delays, target in train_loader:\n",
        "        # one-hot encode sequence\n",
        "        x = idx_to_onehot(seq, k=N+1).float().to(DEVICE)      # (batch, seq_len, N+1)\n",
        "        y = target.to(DEVICE)                                 # (batch, seq_len)\n",
        "        d = delays.to(DEVICE)                                 # (batch,)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x, d)                                  # (batch, seq_len, N+1)\n",
        "        loss = criterion(logits.view(-1, N+1), y.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    acc = test_variable_delay_model(model, seq_length=SEQ_LENGTH)\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {avg_loss:.4f} | Test Acc: {acc*100:.2f}%\")\n",
        "\n",
        "# ----------------------------\n",
        "# Final check\n",
        "# ----------------------------\n",
        "end_time = time.time()\n",
        "duration = end_time - start_time\n",
        "final_acc = test_variable_delay_model(model, seq_length=SEQ_LENGTH)\n",
        "\n",
        "assert duration < 600, f'executing took {duration:.2f}s, which is longer than 10 mins'\n",
        "assert final_acc > 0.99, f'accuracy is too low, got {final_acc:.3f}'\n",
        "print(f\"✅ tests passed | duration {duration:.2f}s | accuracy {final_acc:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Mhk6j9lNCOdt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}